{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":30648,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Pay Attention to MLPs","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19"}},{"cell_type":"markdown","source":"**Executing the concepts outlined in the paper [\"Pay Attention to MLPs\"](https://arxiv.org/pdf/2105.08050v1.pdf).**","metadata":{}},{"cell_type":"markdown","source":"## Introduction","metadata":{}},{"cell_type":"markdown","source":"In our project, we followed the guidelines presented in the research paper titled \"Pay Attention to MLPs\" and applied the neural network structure detailed in the paper to classify image datasets.\n\nOnce our implementation proved successful, we conducted experiments with the CIFAR10 dataset, consisting of 60,000 labeled color images sized 32x32 pixels, categorized into 10 classes (e.g., dog, bird, airplane).\n\nTo start, we will provide a brief overview of the implemented architecture and its associated advantages.","metadata":{}},{"cell_type":"markdown","source":"## Overview of gMLP Architecture","metadata":{}},{"cell_type":"markdown","source":"The paper introduces an architecture named gMLP, designed for vision applications and Natural Language Processing (NLP). It stands out by excluding self-attention modules, providing an alternative perspective for Transformers. The gMLP model primarily relies on basic Multi-Layer Perceptron (MLP) layers and incorporates a gating mechanism.\n\nLet's delve into a summary of how the model operates. The overall structure of the architecture is visually represented in the accompanying image.\n\n<div>\n<img src=\"gMLP.png\" width=\"500\"/>\n</div>\n\n1. **Input Embedding:** The initial step involves transforming the input into a tensor suitable for the network, ensuring the output tensor has a channel dimension of d_model.\n2. **gMLP Blocks:** Lx gMLP blocks follow, connected in series, each with its trainable parameters. These blocks encompass the following steps:\n    * Creating a copy of the input and storing it separately.\n    * Passing the input through a Normalization layer (Layer Normalization).\n    * Channel Projection layer over the channel axis with a Fully Connected Linear Layer having d_ffn output nodes.\n    * Applying Gaussian Error Linear Unit Activation.\n    * Feeding the output into the Spatial Gating Unit.\n        * Splitting the input into two parts along the channel axis (u and v)\n        * Normalizing v\n        * Projecting v spatially\n        * Element-wise multiplying it with u\n    * Passing through another Projection layer over the channel axis.\n    * Adding the shortcut to the output for better training of identity mappings.\n3. **Model Training:** The authors trained the gMLP architecture on the ImageNet dataset and for BERT Masked Language Modeling. For the ImageNet task, three versions of the network were proposed with varying parameters (Lx, d_model, d_ffn, dropout probability), achieving the following accuracies:\n    * gMLP-Ti: 72%\n    * gMLP-S: 79.4%\n    * gMLP-B: 81.6%\n    \nThese results compare favorably with other architectures, demonstrating the effectiveness of the gMLP model.","metadata":{}},{"cell_type":"markdown","source":"## Overview of our Implementation","metadata":{}},{"cell_type":"markdown","source":"Our implementation is organized into distinct sections:\n\n1. **Implementation of gMLP:** This includes module definitions for the key components of gMLP and our adaptation for image classification. Noteworthy modules are SpatialGatingUNIT, gMLPBlock, gMLP, and gMLPImage.\n2. **Supporting Methods:** These methods play a role in various evaluations. Notable functions include \"evaluate_model\" and \"get_accuracy\".\n3. **Training Methods:** Responsible for training our models, these methods are encapsulated in \"loop_vm\".\n4. Sections for Training the Dataset: These sections are further divided into the following subsections:\n    * Set Parameters\n    * Import Dataset and Preparation of DataLoaders\n    * Create the Model with Given Parameters and Move It to Device\n    * Train the Models on the Dataset\n    * Evaluate the Model and Obtain the Confusion Matrix\n\nIn our code, crucial aspects are elucidated through comments. Model evaluation occurs both during training with a validation dataset and after the network has been trained with a test dataset. The outputs include accuracy, precision, recall, and a confusion matrix. The subsequent sections will delve into the actual code segments.","metadata":{}},{"cell_type":"markdown","source":"## Import necessary libraries","metadata":{}},{"cell_type":"code","source":"import torch.nn as nn\nimport torch\nfrom sklearn.metrics import precision_score, recall_score, confusion_matrix\nimport tqdm\nimport numpy as np\nfrom torchvision import transforms, datasets\nfrom torch.utils.data import random_split, DataLoader\nimport matplotlib.pyplot as plt\nimport seaborn as sns","metadata":{"execution":{"iopub.status.busy":"2024-02-13T15:36:46.319818Z","iopub.execute_input":"2024-02-13T15:36:46.320269Z","iopub.status.idle":"2024-02-13T15:36:46.326811Z","shell.execute_reply.started":"2024-02-13T15:36:46.320226Z","shell.execute_reply":"2024-02-13T15:36:46.325653Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Setting the Device","metadata":{}},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")","metadata":{"execution":{"iopub.status.busy":"2024-02-13T15:24:42.672886Z","iopub.execute_input":"2024-02-13T15:24:42.673256Z","iopub.status.idle":"2024-02-13T15:24:42.684373Z","shell.execute_reply.started":"2024-02-13T15:24:42.673225Z","shell.execute_reply":"2024-02-13T15:24:42.683542Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Implementation of gMLP","metadata":{}},{"cell_type":"code","source":"# Definition of the Spatial Gating Unit class\nclass SpatialGatingUnit(nn.Module):\n    def __init__(self, d_ffn, seq_len):\n        super(SpatialGatingUnit, self).__init__()\n\n        # Layer normalization for the second part of the input\n        self.norm = nn.LayerNorm(d_ffn // 2)\n        \n        # Linear projection for the spatial gating\n        self.proj = nn.Linear(seq_len, seq_len)\n        \n        # Initialize weights close to zero and biases as one\n        nn.init.uniform_(self.proj.weight, -0.01, 0.01)\n        nn.init.ones_(self.proj.bias)\n\n    def forward(self, x):\n        # Split input x into two equal parts, u and v, along the channel axis\n        u, v = x.chunk(2, dim=-1)\n        \n        # Perform layer normalization on v\n        v = self.norm(v)\n        \n        # Change the channel with the spatial dimension for v\n        v = v.permute(0, 2, 1)\n        \n        # Perform spatial projection\n        v = self.proj(v)\n        \n        # Change the channel with the spatial dimension back to normal\n        v = v.permute(0, 2, 1)\n        \n        # Return the product of the element-wise multiplied tensors\n        return u * v","metadata":{"execution":{"iopub.status.busy":"2024-02-13T15:24:42.685503Z","iopub.execute_input":"2024-02-13T15:24:42.685784Z","iopub.status.idle":"2024-02-13T15:24:42.697083Z","shell.execute_reply.started":"2024-02-13T15:24:42.685759Z","shell.execute_reply":"2024-02-13T15:24:42.696005Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Definition of the gMLP block class\nclass gMLPBlock(nn.Module):\n    def __init__(self, d_model, d_ffn, seq_len, dropout_prob):\n        super(gMLPBlock, self).__init__()\n\n        # Layer normalization for the input\n        self.norm = nn.LayerNorm(d_model)\n        \n        # Linear projection from d_model to d_ffn along channels\n        self.proj = nn.Linear(d_model, d_ffn)\n        \n        # GELU activation function\n        self.gelu = nn.GELU()\n        \n        # Spatial Gating Unit\n        self.sgu = SpatialGatingUnit(d_ffn, seq_len)\n        \n        # Linear projection from d_ffn/2 to d_model along channels\n        self.proj2 = nn.Linear(d_ffn // 2, d_model)\n        \n        # Dropout probability and Bernoulli distribution for dropout\n        self.prob = torch.Tensor([dropout_prob])\n        self.dist = torch.distributions.bernoulli.Bernoulli(self.prob)\n\n    def forward(self, x):\n        # If training and dropout is chosen, return the input x\n        if self.training and torch.equal(self.dist.sample(), torch.zeros(1)):\n            return x\n        \n        # Save a copy of x for the shortcut connection\n        shortcut = x\n        \n        # Perform layer normalization\n        x = self.norm(x)\n        \n        # Perform projection from d_model to d_ffn along channels\n        x = self.proj(x)\n        \n        # Perform GELU activation\n        x = self.gelu(x)\n        \n        # Perform spatial gating unit\n        x = self.sgu(x)\n        \n        # Perform projection from d_ffn/2 to d_model along channels\n        x = self.proj2(x)\n        \n        # Add shortcut connection\n        return x + shortcut","metadata":{"execution":{"iopub.status.busy":"2024-02-13T15:43:29.032102Z","iopub.execute_input":"2024-02-13T15:43:29.033131Z","iopub.status.idle":"2024-02-13T15:43:29.042582Z","shell.execute_reply.started":"2024-02-13T15:43:29.033088Z","shell.execute_reply":"2024-02-13T15:43:29.041547Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Definition of the gMLP model class\nclass gMLP(nn.Module):\n    def __init__(self, d_model, d_ffn, seq_len, lx, dropout_probs):\n        super(gMLP, self).__init__()\n\n        # Generate a list of dropout probabilities using linear interpolation\n        self.dropout_probs = torch.linspace(dropout_probs[0], dropout_probs[1], lx)\n        \n        # Create a sequential container of gMLP blocks with varying dropout probabilities\n        self.blocks = nn.Sequential(*[gMLPBlock(d_model, d_ffn, seq_len, dropout_prob) for dropout_prob in self.dropout_probs])\n\n    def forward(self, x):\n        # Perform lx sequential blocks of gMLP\n        x = self.blocks(x)\n        \n        return x","metadata":{"execution":{"iopub.status.busy":"2024-02-13T15:24:42.718235Z","iopub.execute_input":"2024-02-13T15:24:42.718704Z","iopub.status.idle":"2024-02-13T15:24:42.730027Z","shell.execute_reply.started":"2024-02-13T15:24:42.718676Z","shell.execute_reply":"2024-02-13T15:24:42.729173Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Definition of the gMLP model for image classification class\nclass gMLPImage(nn.Module):\n    def __init__(self, d_model, d_ffn, lx, patch_size, image_size, channels, classes, dropout_probs):\n        super(gMLPImage, self).__init__()\n        \n        # Calculate the sequence length based on patch size and image dimensions\n        self.seq_len = (image_size // patch_size) ** 2\n        \n        # Patch embedding using a 2D convolutional layer\n        self.patch_embedding = nn.Conv2d(channels, d_model, kernel_size=patch_size, stride=patch_size)\n        \n        # gMLP layer\n        self.gmlp = gMLP(d_model, d_ffn, self.seq_len, lx, dropout_probs)\n        \n        # Output layer for classification\n        self.output = nn.Linear(d_model, classes)\n\n    def forward(self, x):\n        # Perform patch embedding via a 2D convolution\n        x = self.patch_embedding(x)\n        \n        # Rearrange dimensions to (batch_size, height, width, channels)\n        x = x.permute(0, 2, 3, 1)\n        \n        # Flatten into a single dimension of shape (batch_size, height*width, channels)\n        x = x.contiguous().view(x.size(0), -1, x.size(-1))\n        \n        # Perform gMLP\n        x = self.gmlp(x)\n        \n        # Perform global average pooling\n        x = x.mean(1)\n        \n        # Perform projection onto the desired number of output classes\n        x = self.output(x)\n        \n        return x","metadata":{"execution":{"iopub.status.busy":"2024-02-13T15:24:42.731022Z","iopub.execute_input":"2024-02-13T15:24:42.731294Z","iopub.status.idle":"2024-02-13T15:24:42.740829Z","shell.execute_reply.started":"2024-02-13T15:24:42.731270Z","shell.execute_reply":"2024-02-13T15:24:42.739918Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Supporting Methods","metadata":{}},{"cell_type":"code","source":"# Evaluate the model on the test dataset\ndef evaluate_model(net, loader):\n    net.eval()\n    correct = 0\n    total = 0\n    all_labels = []\n    all_predictions = []\n    \n    # Disable gradient computation during evaluation\n    with torch.no_grad():\n        # Iterate through the data loader\n        for images, labels in tqdm.tqdm(loader, desc=\"Evaluation\"):\n            # Move images and labels to the device\n            images = images.to(device)\n            labels = labels.to(device)\n            \n            # Forward pass to get predictions\n            outputs = net(images)\n            \n            # Compute accuracy\n            _, predicted = torch.max(outputs.data, 1)\n            total += labels.size(0)\n            correct += (predicted == labels).sum().item()\n            \n            # Collect labels and predictions for further analysis\n            all_labels.extend(labels.cpu().numpy())\n            all_predictions.extend(predicted.cpu().numpy())\n            \n    # Calculate evaluation metrics\n    accuracy = 100 * correct / total\n    precision = precision_score(all_labels, all_predictions, average='micro')\n    recall = recall_score(all_labels, all_predictions, average='micro')\n    conf_matrix = confusion_matrix(all_labels, all_predictions)\n    \n    return accuracy, precision, recall, conf_matrix","metadata":{"execution":{"iopub.status.busy":"2024-02-13T15:24:42.742159Z","iopub.execute_input":"2024-02-13T15:24:42.742389Z","iopub.status.idle":"2024-02-13T15:24:42.768050Z","shell.execute_reply.started":"2024-02-13T15:24:42.742368Z","shell.execute_reply":"2024-02-13T15:24:42.767131Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Calculate accuracy given predictions and true labels\ndef get_accuracy(preds, y):\n    # Get the index of the maximum predicted value along the second dimension\n    preds = preds.argmax(dim=1, keepdim=True)\n    \n    # Check if the predicted index matches the true labels\n    correct = preds.squeeze(1).eq(y)\n    \n    # Calculate accuracy as the proportion of correct predictions\n    acc = correct.sum() / torch.FloatTensor([y.shape[0]]).to(device)\n    \n    return acc","metadata":{"execution":{"iopub.status.busy":"2024-02-13T15:24:42.769450Z","iopub.execute_input":"2024-02-13T15:24:42.769744Z","iopub.status.idle":"2024-02-13T15:24:42.779497Z","shell.execute_reply.started":"2024-02-13T15:24:42.769721Z","shell.execute_reply":"2024-02-13T15:24:42.778574Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Training Methods","metadata":{}},{"cell_type":"code","source":"# Define CrossEntropyLoss as the loss function\nloss_fn = nn.CrossEntropyLoss()","metadata":{"execution":{"iopub.status.busy":"2024-02-13T15:24:42.781919Z","iopub.execute_input":"2024-02-13T15:24:42.782201Z","iopub.status.idle":"2024-02-13T15:24:42.793475Z","shell.execute_reply.started":"2024-02-13T15:24:42.782176Z","shell.execute_reply":"2024-02-13T15:24:42.792566Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Training/validation loop for a neural network\ndef loop_vm(net, loader, opt, is_train):\n    # Set the network to train or evaluation mode\n    net.train(is_train)\n    \n    # Lists to store losses and accuracies\n    losses = []\n    accs = []\n    \n    # Progress bar for visualization\n    pbar = tqdm.tqdm(loader, total=len(loader))\n    \n    # Iterate through the data loader\n    for x, y in pbar:\n        # Move data to the device\n        x = x.to(device)\n        y = y.to(device)\n        \n        # Enable or disable gradient computation based on training mode\n        with torch.set_grad_enabled(is_train):\n            # Forward pass to get predictions\n            preds = net(x)\n            \n            # Calculate the loss using the defined loss function\n            loss = loss_fn(preds, y)\n            \n            # Calculate accuracy using the get_accuracy function\n            acc = get_accuracy(preds, y)\n            \n            # Append loss and accuracy to the respective lists\n            losses.append(loss.item())\n            accs.append(acc.item())\n            \n        # Backward pass and optimization step during training\n        if is_train:\n            opt.zero_grad()\n            loss.backward()\n            opt.step()\n            \n        # Update progress bar description with current statistics\n        if is_train:\n            pbar.set_description(f'Training  : Epoch: {epoch+1} Loss: {np.mean(losses):.4f} Acc: {np.mean(accs):.4f}')\n        else:\n            pbar.set_description(f'Validation: Epoch: {epoch+1} Loss: {np.mean(losses):.4f} Acc: {np.mean(accs):.4f}')","metadata":{"execution":{"iopub.status.busy":"2024-02-13T15:24:42.794488Z","iopub.execute_input":"2024-02-13T15:24:42.794746Z","iopub.status.idle":"2024-02-13T15:24:42.805616Z","shell.execute_reply.started":"2024-02-13T15:24:42.794723Z","shell.execute_reply":"2024-02-13T15:24:42.804802Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Set Parameters","metadata":{}},{"cell_type":"code","source":"epochs = 6                  # Number of epochs to complete\nd_model = 220               # Dimensionality of the model\nd_ffn = 900                 # Dimensionality of the feedforward network within the model\nlx = 6                      # Number of blocks to be sequentially executed\npatch_size = 2              # Patch size for patch embedding convolution\nimage_size = 32             # Size of the input images (must be square)\nclasses = 10                # Number of output classes\nchannels = 3                # Number of input channels (in our case, RGB)\ndropout_probs = [0.9, 0.9]  # Initial and final values of dropout probabilities, used to create a linspace for each block\nbatch_size = 64             # Batch size for gradient descent\neval_ratio = 0.2            # Ratio of the dataset used for validation","metadata":{"execution":{"iopub.status.busy":"2024-02-13T15:24:42.806732Z","iopub.execute_input":"2024-02-13T15:24:42.807178Z","iopub.status.idle":"2024-02-13T15:24:42.817179Z","shell.execute_reply.started":"2024-02-13T15:24:42.807145Z","shell.execute_reply":"2024-02-13T15:24:42.816309Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Import dataset and prepare the DataLoaders","metadata":{}},{"cell_type":"code","source":"# Define a set of image transformations, including resizing and conversion to tensor\nT = transforms.Compose([\n    transforms.Resize((image_size, image_size)),\n    transforms.ToTensor()\n])\n\n# Load the CIFAR-10 dataset for training, testing, and validation\nCIFAR10_train = datasets.CIFAR10(\"data/\", train=True, download=True, transform=T)\nclass_names = CIFAR10_train.classes\nCIFAR10_test = datasets.CIFAR10(\"data/\", train=False, download=True, transform=T)\n\n# Split the training dataset into training and validation sets\nCIFAR10_train, CIFAR10_val = random_split(CIFAR10_train, [int(len(CIFAR10_train) * (1 - eval_ratio)), int(len(CIFAR10_train) * eval_ratio)])\n\n# Create data loaders for training, testing, and validation sets\ntrain_loader = DataLoader(CIFAR10_train, batch_size=batch_size, shuffle=True, num_workers=2, pin_memory=True)\ntest_loader = DataLoader(CIFAR10_test, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)\nval_loader = DataLoader(CIFAR10_val, batch_size=batch_size, shuffle=False, num_workers=2, pin_memory=True)","metadata":{"execution":{"iopub.status.busy":"2024-02-13T15:24:42.818386Z","iopub.execute_input":"2024-02-13T15:24:42.818679Z","iopub.status.idle":"2024-02-13T15:24:44.482716Z","shell.execute_reply.started":"2024-02-13T15:24:42.818656Z","shell.execute_reply":"2024-02-13T15:24:44.481892Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Create the model with given parameters and move it to device","metadata":{}},{"cell_type":"code","source":"# Create an instance of the gMLPImage model\ngmlp = gMLPImage(\n    d_model = d_model,\n    d_ffn = d_ffn,\n    lx = lx,\n    patch_size = patch_size,\n    image_size = image_size,\n    channels = channels,\n    classes = classes,\n    dropout_probs = dropout_probs\n).to(device)","metadata":{"execution":{"iopub.status.busy":"2024-02-13T15:24:44.483872Z","iopub.execute_input":"2024-02-13T15:24:44.484165Z","iopub.status.idle":"2024-02-13T15:24:44.524512Z","shell.execute_reply.started":"2024-02-13T15:24:44.484141Z","shell.execute_reply":"2024-02-13T15:24:44.523737Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Train the models on the the dataset","metadata":{}},{"cell_type":"code","source":"# Set up the Adam optimizer for the gMLPImage model parameters\nopt = torch.optim.Adam(gmlp.parameters())\n\n# Training loop over a specified number of epochs\nfor epoch in range(epochs):\n    # Training phase: Execute training loop on the training dataset\n    loop_vm(gmlp, train_loader, opt, True)\n    \n    # Validation phase: Execute evaluation loop on the validation dataset\n    loop_vm(gmlp, val_loader, opt, False)","metadata":{"execution":{"iopub.status.busy":"2024-02-13T15:24:44.525803Z","iopub.execute_input":"2024-02-13T15:24:44.526416Z","iopub.status.idle":"2024-02-13T15:32:57.949600Z","shell.execute_reply.started":"2024-02-13T15:24:44.526366Z","shell.execute_reply":"2024-02-13T15:32:57.948546Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Evaluate the model and get the confusion matrix","metadata":{}},{"cell_type":"code","source":"# Evaluate the gMLPImage model on the test dataset\ntest_accuracy, test_precision, test_recall, conf_matrix = evaluate_model(gmlp, test_loader)\n\n# Print the evaluation metrics\nprint(f\"Test Accuracy: {test_accuracy:.2f}%\\nTest Precision: {test_precision * 100:.2f}%\\nTest Recall: {test_recall * 100:.2f}%\")\n\n# Plot the confusion matrix\nplt.figure(figsize=(6, 4))\nsns.heatmap(conf_matrix, annot=True, fmt='d',xticklabels=class_names, yticklabels=class_names)\nplt.xlabel('Predicted')\nplt.ylabel('True')\nplt.show()","metadata":{"execution":{"iopub.status.busy":"2024-02-13T15:32:57.951322Z","iopub.execute_input":"2024-02-13T15:32:57.952199Z","iopub.status.idle":"2024-02-13T15:33:06.123638Z","shell.execute_reply.started":"2024-02-13T15:32:57.952155Z","shell.execute_reply":"2024-02-13T15:33:06.122568Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Conclusion","metadata":{}},{"cell_type":"markdown","source":"Despite the apparent simplicity of the overall gMLP architecture, fully understanding its intricacies required considerable time and numerous iterations of analysis. Existing implementations mentioned in the section below proved invaluable in clarifying specific details of the algorithm that might have been challenging to comprehend solely from the paper.\n\nDuring testing, we encountered overfitting issues after approximately 8 epochs. To address this, we introduced dropout for the gMLP blocks, resulting in a slight improvement in results, although the impact was not substantial. Additional dropout mechanisms at different stages of the network and similar strategies could be explored as potential remedies against overfitting.\n\nThrough parameter adjustments, we successfully enhanced our results on datasets, initially yielding suboptimal numbers to more satisfactory outcomes in the end. Despite these improvements, there remains ample room for further refinement of the network. For instance, reevaluating image embeddings and employing data augmentation to augment dataset size could be beneficial. Additionally, running the network on more robust hardware and exploring other methods to mitigate overfitting are avenues worth exploring for future enhancements.","metadata":{}},{"cell_type":"markdown","source":"## References","metadata":{}},{"cell_type":"markdown","source":"References for Building this Notebook:\n\n1. Paper: [Pay Attention to MLPs](https://arxiv.org/pdf/2105.08050v1.pdf)\n2. [g-mlp-pytorch](https://github.com/lucidrains/g-mlp-pytorch)\n3. [Pay-Attention-to-MLPs](https://github.com/antonyvigouret/Pay-Attention-to-MLPs)\n4. [LabML](https://nn.labml.ai/transformers/gmlp/index.html)\n\nThese resources played a crucial role in informing our understanding and implementation of the gMLP architecture.","metadata":{}}]}